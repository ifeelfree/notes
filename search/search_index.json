{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Technical Note","text":"<p>I use this technical note to accumulate all the knowledge I have obtained when working on different projects.</p>"},{"location":"#1-computer-languages","title":"1. Computer Languages","text":""},{"location":"#11-python","title":"1.1. Python","text":"<p>Life is short, and please use Python!</p>"},{"location":"#12-markdown","title":"1.2. Markdown","text":"<p>Markdown's goal is to allow people \u201cto write using an  easy-to-read, easy-to-write plain text format, and optionally  convert it to structurally valid XHTML (or HTML).\u201d </p>"},{"location":"#13-cuda","title":"1.3 CUDA","text":"<p>CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model that lets developers use Graphics Processing Units (GPUs) for general-purpose processing.</p>"},{"location":"#14-cmake","title":"1.4 CMake","text":"<p>CMake is cross-platform free and open-source software for build automation, testing, packaging and installation of software by using a compiler-independent method.</p>"},{"location":"#15-c","title":"1.5 C++","text":""},{"location":"#2-computer","title":"2. Computer","text":""},{"location":"#21-linux","title":"2.1 Linux","text":"<p>The first step in becoming a data scientist: forget about Windows.</p>"},{"location":"#22-windows","title":"2.2 Windows","text":"<p>Windows is a computer operating system (OS) developed by Microsoft Corporation </p>"},{"location":"#23-docker","title":"2.3 Docker","text":"<p>Docker is an open platform for developing, shipping, and running applications.</p>"},{"location":"#24-popular-apps","title":"2.4 Popular Apps","text":"<p>Popular Apps include web browsing, office suites, media, creative work, productivity, and system tools. </p>"},{"location":"#3-software-development","title":"3. Software Development","text":""},{"location":"#31-version-control","title":"3.1 Version Control","text":"<p>Version control is used to track and manage changes to files, especially code, over time.</p>"},{"location":"#32-design-pattern","title":"3.2 Design Pattern","text":"<p>A software design pattern or design pattern is a general, reusable solution to a commonly occurring problem in many contexts in software design.</p>"},{"location":"#4-machine-learning","title":"4. Machine Learning","text":""},{"location":"#41-data","title":"4.1 Data","text":"<p>Data is the new oil.</p>"},{"location":"#42-mlops","title":"4.2 Mlops","text":"<p>MLOps stands for Machine Learning Operations. MLOps is a core function of Machine Learning engineering, focused on streamlining the process of taking machine learning models to production, and then maintaining and monitoring them.</p>"},{"location":"#43-model","title":"4.3 Model","text":"<p>A machine learning model is a file that has  been trained to recognize certain types of patterns. </p>"},{"location":"#44-tensorflow","title":"4.4 TensorFlow","text":"<p>TensorFlow is a popular deep learning framework widely used in the industry. </p>"},{"location":"#5-sensors","title":"5. Sensors","text":""},{"location":"#51-3d-sensing","title":"5.1 3D Sensing","text":"<p>3D sensing is a depth-sensing technology that augments camera capabilities for facial and object recognition. The process of capturing a real-world object's length, width, and height with more clarity and in-depth detail than can be achieved using a number of different technologies.</p>"},{"location":"#52-sensor-fusion","title":"5.2 Sensor Fusion","text":"<p>Sensor fusion integrates data from various sensors to enable smarter decision-making by computer systems.</p>"},{"location":"#6-low-level-computer-vision","title":"6. Low-level Computer Vision","text":""},{"location":"#61-image-construction","title":"6.1 Image Construction","text":"<p>Image construction refers to the  computational process of generating images from raw data collected by sensors.</p>"},{"location":"#62-image-quality-assessment","title":"6.2 Image Quality Assessment","text":"<p>Image quality assessment refers to the methods used to measure and evaluate the perceived quality of digital images,</p>"},{"location":"#7-geometric-computer-vision","title":"7. Geometric Computer Vision","text":""},{"location":"#71-projective-distortion-correction","title":"7.1 Projective Distortion Correction","text":"<p>Projective distortion correction is the process of digitally manipulating an image to remove the geometric distortions caused when a 3D scene is projected onto a 2D image plane from an angled viewpoint.</p>"},{"location":"#72-slam","title":"7.2 SLAM","text":"<p>SLAM (Simultaneous Localization and Mapping) is a fundamental technology that enables an autonomous system (like a robot or drone) to build a map of an unknown environment while simultaneously determining its own precise location within that map in real-time.</p>"},{"location":"#8-image-analysis","title":"8. Image Analysis","text":""},{"location":"#81-texture-analysis","title":"8.1 Texture Analysis","text":"<p>Texture analysis in image processing is the process of quantifying and characterizing the spatial arrangement and variation of pixel intensities within a digital image.</p>"},{"location":"#82-image-segmentation","title":"8.2 Image Segmentation","text":"<p>Image segmentation is a core computer vision technique that involves partitioning a digital image into multiple segments, or groups of pixels, to simplify its representation and make it easier to analyze.</p>"},{"location":"#83-object-detection","title":"8.3 Object Detection","text":"<p>Object detection is a computer vision technique that allows  us to identify and locate objects in an image or video.</p>"},{"location":"#9-video-analysis","title":"9. Video Analysis","text":""},{"location":"#91-hand-pose-estimation","title":"9.1 Hand Pose Estimation","text":"<p>Posture estimation is used to understand human's behavior. </p>"},{"location":"#92-object-tracking","title":"9.2 Object Tracking","text":"<p>Video object tracking is a key computer vision task that involves locating a moving object (or multiple objects) over time in a sequence of video frames. </p>"},{"location":"#10-inspiring-life","title":"10. Inspiring Life","text":""},{"location":"#101-english","title":"10.1 English","text":"<p>English is everywhere. </p>"},{"location":"#102-french","title":"10.2 French","text":"<p>C'est belly langue. </p>"},{"location":"#103-poems","title":"10.3 Poems","text":"<p>Poetry is a form of literary art that uses the aesthetic and often rhythmic qualities of language to evoke meanings and emotions beyond literal definitions. </p>"},{"location":"#104-cooking","title":"10.4 Cooking","text":"<p>I like cooking for my families. </p>"},{"location":"#105-health","title":"10.5 Health","text":"<p>Health is everything.</p>"},{"location":"#11-resource","title":"11. Resource","text":""},{"location":"#111-course","title":"11.1 Course","text":""},{"location":"computer/linux/ubuntu/","title":"Ubuntu","text":""},{"location":"computer/linux/ubuntu/#ubuntu-commands","title":"Ubuntu Commands","text":"<ul> <li> <p>Linux Commands Series 1: Software Management</p> </li> <li> <p>Linux Commands Series 2: Managing Shared Libraries</p> </li> <li> <p>Linux Commands Series 3: A Guide to Disk/File Management on\u00a0Ubuntu</p> </li> </ul>"},{"location":"computer/linux/ubuntu/#ubuntu-apps","title":"Ubuntu Apps","text":"<ul> <li> <p>Popular Apps on Ubuntu: FreeTube-A Private YouTube Client for\u00a0Ubuntu</p> </li> <li> <p>How to Use Rclone to Manage Google Drive on Ubuntu 24.04</p> </li> </ul>"},{"location":"computer/linux/ubuntu/#ubuntu-helpdesk","title":"Ubuntu Helpdesk","text":"<ul> <li>How I Solved the \u201cBluetooth Stuck in Searching\u201d Problem on Ubuntu 24.04</li> </ul>"},{"location":"computer/popularapps/mkdocs/","title":"Mkdocs","text":"<ul> <li>How to Generate Mkdocs on Github Using Docker</li> </ul>"},{"location":"languages/cplusplus/cplusplus/","title":"C++","text":""},{"location":"languages/cplusplus/cplusplus/#new-features","title":"New features","text":"<ul> <li>Understanding qobject_cast, dynamic_cast, and static_cast in Qt A Practical Comparison with a Minimal Example</li> </ul>"},{"location":"languages/cplusplus/cplusplus/#build-libraries","title":"Build libraries","text":"<ul> <li>The Night I Finally Understood Linux Libraries</li> </ul>"},{"location":"languages/cplusplus/cplusplus/#qt","title":"QT","text":"<ul> <li>Deploying a Qt 6 Development Environment Using Conan, CMake, and Visual Studio</li> <li> <p>Introduction to Qt Signal and Slot Mechanism: Based on a Simple Radio\u2013Station Example</p> </li> <li> <p>Qt\u2019s QObject Memory Management: How Parent\u2013child Hierarchies Delete Objects for You</p> </li> <li> <p>Introduction to Q_PROPERTY in Qt A Practical Example Using QObject, Properties, and Notifications</p> </li> </ul>"},{"location":"languages/cuda/cuda/","title":"Learning CUDA","text":"<ul> <li> <p>Technical Notes on Introduction to Parallel Programming with CUDA on Coursera: (1) A Simple CUDA Example</p> </li> <li> <p>Technical Notes on Introduction to Parallel Programming with CUDA on Coursera: (2) CUDA Introduction</p> </li> </ul>"},{"location":"languages/cuda/cuda_algorithm/","title":"Cuda algorithm","text":""},{"location":"languages/cuda/cuda_algorithm/#cuda-algorithms","title":"CUDA Algorithms","text":""},{"location":"languages/cuda/cuda_algorithm/#transformation","title":"Transformation","text":""},{"location":"languages/cuda/cuda_algorithm/#reduction","title":"Reduction","text":"<ul> <li>max reduction.cu</li> </ul>"},{"location":"languages/cuda/cuda_basics/","title":"Cuda basics","text":""},{"location":"languages/cuda/cuda_basics/#cuda-fundamentals","title":"CUDA Fundamentals","text":""},{"location":"languages/cuda/cuda_basics/#gpucpu","title":"GPU/CPU","text":""},{"location":"languages/cuda/cuda_basics/#gpu","title":"GPU","text":""},{"location":"languages/cuda/cuda_basics/#gpucpu_1","title":"GPU/CPU","text":""},{"location":"languages/cuda/cuda_basics/#what-makes-gpu-different-from-cpu","title":"What makes GPU different from CPU?","text":"<p>GPUs rely more on latency hiding than CPUs to achieve high performance.</p>"},{"location":"languages/cuda/cuda_basics/#gpu-and-cpu-architectural-difference","title":"GPU and CPU architectural difference","text":"Feature CPU GPU Threads Few Thousands Caches Large Small Latency strategy Minimize Hide Context switch Expensive Cheap Best for Low-latency tasks High-throughput tasks Memory bandwidth Low High"},{"location":"languages/cuda/cuda_basics/#heterogeneous-programming-model","title":"Heterogeneous programming model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    kernels / memcpy    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CPU (Host) \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6  \u2502 GPU (Device) \u2502\n\u2502  Control &amp;   \u2502                       \u2502 Parallel     \u2502\n\u2502  Launch      \u2502 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502 Execution    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        results         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n</code></pre> <p>CPU to manage and launch work while offloading massively parallel computations to the GPU for execution</p> <p>cuda demo: the GPU kenerl is marked with the <code>__global__</code> qualifier.</p> <pre><code>\n__global__ void vector_add(const float *A, const float *B, float *C, int ds){\n\n  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n  if (idx &lt; ds)\n    C[idx] = A[idx] + B[idx];\n}\n\nvoid main(){\n    float *a, *b, *out;\n    float *d_a;\n    a = (float*)malloc(sizeof(float) * N);\n    // Allocate device memory for a\n    cudaMalloc((void**)&amp;d_a, sizeof(float) * N);\n    // Transfer data from host to device memory\n    cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);  \n    vector_add&lt;&lt;&lt;1,1&gt;&gt;&gt;(out, d_a, b, N);\n    // Cleanup after kernel execution\n    cudaFree(d_a);\n    free(a);\n}\n</code></pre> <p>GPU kernel is compiled with nvcc</p> <ul> <li> <p><code>__global__</code>: running kernels from the host</p> </li> <li> <p><code>__device__</code>: running kernels from the device</p> </li> </ul> <p>CPU codes are compiled with gcc, cl.exe</p> <ul> <li><code>__host__</code>: running codes from the host</li> </ul>"},{"location":"languages/cuda/cuda_basics/#other-examples","title":"Other examples","text":"<ul> <li>matrix multiplication.cu</li> </ul>"},{"location":"languages/cuda/cuda_basics/#gpu_1","title":"GPU","text":""},{"location":"languages/cuda/cuda_basics/#gpu-properities","title":"GPU properities","text":"<pre><code>#include &lt;stdio.h&gt; \n\nint main() {\n  int nDevices;\n\n  cudaGetDeviceCount(&amp;nDevices);\n  for (int i = 0; i &lt; nDevices; i++) {\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&amp;prop, i);\n    printf(\"Device Number: %d\\n\", i);\n    printf(\"  Device name: %s\\n\", prop.name);\n    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n           prop.memoryClockRate);\n    printf(\"  Memory Bus Width (bits): %d\\n\",\n           prop.memoryBusWidth);\n    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n  }\n}\n</code></pre>"},{"location":"languages/cuda/cuda_basics/#compute-capabilites","title":"Compute capabilites","text":"Feature Tesla C870 Tesla C1060 Tesla C2050 Tesla K10 Tesla K20 Compute Capability 1.0 1.3 2.0 3.0 3.5 Max Threads per Thread Block 512 512 1024 1024 1024 Max Threads per SM 768 1024 1536 2048 2048 Max Thread Blocks per SM 8 8 8 16 16 <ul> <li> <p>CUDA GPU Compute Capability</p> </li> <li> <p>Legacy CUDA GPU Compute Capability</p> </li> <li> <p>GPU Comparison Guides</p> </li> </ul>"},{"location":"languages/cuda/cuda_extension/","title":"Cuda extension","text":""},{"location":"languages/cuda/cuda_extension/#cuda-extensions","title":"CUDA Extensions","text":""},{"location":"languages/cuda/cuda_extension/#nvcc","title":"nvcc","text":"<p><code>nvcc vector_add.cu -o vector_add</code></p>"},{"location":"languages/cuda/cuda_extension/#nvprof","title":"nvprof","text":"<p><code>nvprof ./vector_add</code></p> <pre><code>==6326== Profiling application: ./vector_add\n==6326== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 97.55%  1.42529s         1  1.42529s  1.42529s  1.42529s  vector_add(float*, float*, float*, int)\n  1.39%  20.318ms         2  10.159ms  10.126ms  10.192ms  [CUDA memcpy HtoD]\n  1.06%  15.549ms         1  15.549ms  15.549ms  15.549ms  [CUDA memcpy DtoH]\n\n</code></pre>"},{"location":"languages/cuda/cuda_extension/#part-2-cuda-libraries","title":"Part 2: CUDA Libraries","text":""},{"location":"languages/cuda/cuda_extension/#thrust","title":"Thrust","text":"<p>Thrust is a powerful library of parallel algorithms and data structures.</p>"},{"location":"languages/cuda/cuda_extension/#cub","title":"cub","text":"<p>CUB provides state-of-the-art, reusable software components for every layer of the CUDA programming model.</p>"},{"location":"languages/cuda/cuda_memory/","title":"Cuda memory","text":""},{"location":"languages/cuda/cuda_memory/#cuda-memory","title":"CUDA Memory","text":""},{"location":"languages/cuda/cuda_memory/#gpucpu-memory","title":"GPU/CPU Memory","text":""},{"location":"languages/cuda/cuda_memory/#cpu-memory-vs-gpu-memory","title":"CPU Memory vs GPU Memory","text":""},{"location":"languages/cuda/cuda_memory/#host-and-device-memory-transfer","title":"Host and device memory transfer","text":"<pre><code>cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n\ncudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n</code></pre> <ul> <li> <p>data transfer between CPU and GPU </p> </li> <li> <p>We need to call <code>cudaDeviceSynchronize()</code> to ensure all GPU operations are completed before copying data from the GPU to the CPU; however, synchronization is not required when copying data from the CPU to the GPU.</p> </li> <li> <p>We use <code>cudaMalloc</code> and <code>cudaFree</code> to manage GPU memories.</p> </li> </ul>"},{"location":"languages/cuda/cuda_memory/#global-memory","title":"Global Memory","text":"<ul> <li>Loads:</li> <li>Cache as default</li> <li>Sequence: L1 cache, L2 cache, GMEM</li> <li>The granularity is 128-byte  </li> <li> <p>It is also possible to use the non-cache mode, and in this case the granularity is 32-byte</p> </li> <li> <p>Stores:</p> </li> <li>Invalidate L1, write back L2 </li> </ul>"},{"location":"languages/cuda/cuda_memory/#coalescing-in-gmem","title":"Coalescing in GMEM","text":"<pre><code>\n// matrix row-sum kernel\n__global__ void row_sums(const float *A, float *sums, size_t ds){\n  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create typical 1D thread index from built-in variables\n  if (idx &lt; ds){\n    float sum = 0.0f;\n    for (size_t i = 0; i &lt; ds; i++)\n      sum += A[idx*ds+i];        \n    sums[idx] = sum;\n}}\n\n</code></pre>"},{"location":"languages/cuda/cuda_memory/#non-coalescing-in-gmem","title":"Non-Coalescing in GMEM","text":"<pre><code>\n__global__ void column_sums(const float *A, float *sums, size_t ds){\n  int idx = threadIdx.x+blockDim.x*blockIdx.x;  \n  if (idx &lt; ds){\n    float sum = 0.0f;\n    for (size_t i = 0; i &lt; ds; i++)\n      sum += A[idx+ds*i];    \n    sums[idx] = sum;\n}}\n</code></pre>"},{"location":"languages/cuda/cuda_memory/#shared-memory","title":"Shared Memory","text":"<ul> <li> <p>threads share data via shared memory in the thread block</p> </li> <li> <p>extremely fast to access shared memory compared to L1 cache, L2 cache and global memory </p> </li> <li> <p>comparable with registers </p> </li> <li> <p>data in shared memory is declared as <code>__shared__</code></p> </li> <li> <p>shared memory is not shared with other thread blocks </p> </li> </ul> <p>Shared Memory Demo</p> <p></p> <pre><code>\n// Synchronize (ensure all the data is available)\n__syncthreads();\n\n// Apply the stencil\nint result = 0;\nfor (int offset = -RADIUS; offset &lt;= RADIUS; offset++)\n   result += temp[lindex + offset];\n\n// Store the result\nout[gindex] = result;\n\n</code></pre> <ul> <li>all the threads must reach this barrier <code>__syncthreads()</code>.</li> </ul> <p>Shared Memory Demo: Matrix Multiplication</p> <pre><code>const int DSIZE = 8192;\nconst int block_size = 32;  // CUDA maximum is 1024 *total* threads in block\nconst float A_val = 3.0f;\nconst float B_val = 2.0f;\n\n// matrix multiply (naive) kernel: C = A * B\n__global__ void mmul(const float *A, const float *B, float *C, int ds) {\n\n  // declare cache in shared memory\n  __shared__ float As[block_size][block_size];\n  __shared__ float Bs[block_size][block_size];\n\n  int idx = threadIdx.x+blockDim.x*blockIdx.x; // create thread x index\n  int idy = threadIdx.y+blockDim.y*blockIdx.y; // create thread y index\n\n  if ((idx &lt; ds) &amp;&amp; (idy &lt; ds)){\n    float temp = 0;\n    for (int i = 0; i &lt; ds/block_size; i++) {\n\n      // Load data into shared memory\n      As[threadIdx.y][threadIdx.x] = A[idy * ds + (i * block_size + threadIdx.x)];\n      Bs[threadIdx.y][threadIdx.x] = B[(i * block_size + threadIdx.y) * ds + idx];\n\n      // Synchronize\n      __syncthreads();\n</code></pre> <pre><code> // Keep track of the running sum\n      for (int k = 0; k &lt; block_size; k++)\n        temp += As[threadIdx.y][k] * Bs[k][threadIdx.x]; // dot product of row and column\n      __syncthreads();\n    }\n    // Write to global memory\n    C[idy*ds+idx] = temp;\n  }\n}\n</code></pre> <ul> <li> <p>First barrier: make sure shared memory is fully written</p> </li> <li> <p>Second barrier: make sure shared memory is fully read before it is reused. </p> </li> <li> <p>matrix multiplication with shared memory.cu</p> </li> </ul> <p>Shared Memory Demo: Matrix Row Summrization</p> <pre><code>\n__global__ void row_sums(const float *A, float *sums, size_t ds){\n\n  int idx = blockIdx.x; // our block index becomes our row indicator\n  if (idx &lt; ds){\n     __shared__ float sdata[block_size];\n     int tid = threadIdx.x;\n     sdata[tid] = 0.0f;\n     size_t tidx = tid;\n\n     while (tidx &lt; ds) {  // block stride loop to load data\n        sdata[tid] += A[idx*ds+tidx];\n        tidx += blockDim.x;  \n        }\n\n     for (unsigned int s=blockDim.x/2; s&gt;0; s&gt;&gt;=1) {\n        __syncthreads();\n        if (tid &lt; s)  // parallel sweep reduction\n            sdata[tid] += sdata[tid + s];\n        }\n     if (tid == 0) sums[idx] = sdata[0];\n  }\n}\n</code></pre> <pre><code>const size_t DSIZE = 16384;      // matrix side dimension\nconst int block_size = 256;  // CUDA maximum is 1024\n\nfloat *h_A, *h_sums, *d_A, *d_sums;\nh_A = new float[DSIZE*DSIZE];  // allocate space for data in host memory\nh_sums = new float[DSIZE]();\nfor (int i = 0; i &lt; DSIZE*DSIZE; i++)  // initialize matrix in host memory\n  h_A[i] = 1.0f;\ncudaMalloc(&amp;d_A, DSIZE*DSIZE*sizeof(float));  // allocate device space for A\ncudaMalloc(&amp;d_sums, DSIZE*sizeof(float));  // allocate device space for vector d_sums\ncudaCheckErrors(\"cudaMalloc failure\"); // error checking\n// copy matrix A to device:\ncudaMemcpy(d_A, h_A, DSIZE*DSIZE*sizeof(float), cudaMemcpyHostToDevice);\ncudaCheckErrors(\"cudaMemcpy H2D failure\");\n//cuda processing sequence step 1 is complete\nrow_sums&lt;&lt;&lt;DSIZE, block_size&gt;&gt;&gt;(d_A, d_sums, DSIZE);\n</code></pre> <p>Demo Shared Memory and Global Memory Comparison</p> <p>matrix row/col sum.cu</p>"},{"location":"languages/cuda/cuda_memory/#atomics","title":"Atomics","text":"<pre><code>const size_t N = 8ULL*1024ULL*1024ULL;   \nconst int BLOCK_SIZE = 256;   \n__global__ void atomic_red(const float *gdata, float *out){\n  size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n  if (idx &lt; N) atomicAdd(out, gdata[idx]);\n}\n\ncudaMemset(d_sum, 0, sizeof(float));\n\ncudaCheckErrors(\"cudaMemset failure\");\n\natomic_red&lt;&lt;&lt;(N+BLOCK_SIZE-1)/BLOCK_SIZE, BLOCK_SIZE&gt;&gt;&gt;(d_A, d_sum);\n\n</code></pre> <pre><code>\n__global__ void reduce_a(float *gdata, float *out){\n     __shared__ float sdata[BLOCK_SIZE];\n     int tid = threadIdx.x;\n     sdata[tid] = 0.0f;\n     size_t idx = threadIdx.x+blockDim.x*blockIdx.x;\n\n     while (idx &lt; N) {  // grid stride loop to load data\n        sdata[tid] += gdata[idx];\n        idx += gridDim.x*blockDim.x;  \n        }\n\n     for (unsigned int s=blockDim.x/2; s&gt;0; s&gt;&gt;=1) {\n        __syncthreads();\n        if (tid &lt; s)  // parallel sweep reduction\n            sdata[tid] += sdata[tid + s];\n        }\n     if (tid == 0) atomicAdd(out, sdata[0]);\n  }\n\n</code></pre>"},{"location":"languages/cuda/cuda_memory/#warp-shuffle","title":"Warp Shuffle","text":"<pre><code>__global__ void reduce_ws(float *gdata, float *out){\n     __shared__ float sdata[32];\n     int tid = threadIdx.x;\n     int idx = threadIdx.x+blockDim.x*blockIdx.x;\n     float val = 0.0f;\n     unsigned mask = 0xFFFFFFFFU;\n     int lane = threadIdx.x % warpSize;\n     int warpID = threadIdx.x / warpSize;\n     while (idx &lt; N) {   \n        val += gdata[idx];\n        idx += gridDim.x*blockDim.x;  \n        }\n\n    for (int offset = warpSize/2; offset &gt; 0; offset &gt;&gt;= 1) \n       val += __shfl_down_sync(mask, val, offset);\n    if (lane == 0) sdata[warpID] = val;\n   __syncthreads(); // put warp results in shared mem\n\n    if (warpID == 0){\n       val = (tid &lt; blockDim.x/warpSize)?sdata[lane]:0;\n       for (int offset = warpSize/2; offset &gt; 0; offset &gt;&gt;= 1) \n          val += __shfl_down_sync(mask, val, offset);\n       if  (tid == 0) atomicAdd(out, val);\n     }\n}\n</code></pre>"},{"location":"languages/cuda/cuda_memory/#unifiedmanaged-memory","title":"Unified/Managed Memory","text":"<ul> <li> <p>Allocate memory once with cudaMallocManaged</p> </li> <li> <p>The same pointer is valid on both CPU and GPU</p> </li> <li> <p>The system automatically migrates pages between CPU and GPU memory on demand</p> </li> <li> <p>Oversubscribe GPU memory (Pascal+). </p> </li> <li> <p>CPU/GPU memory conherence. </p> </li> </ul> <p> </p> <ul> <li> <p>Managed Memory Add.cu</p> </li> <li> <p>UM is first and foremost about ease of programming and programmer producitivity. </p> </li> </ul> <p>oversubscription</p> <p>deep copy</p> <p></p> <ul> <li>Managed memory List.cu</li> </ul> <pre><code>class Managed {\npublic:\n    void * operator new(size_t len){\n        void *ptr;\n        cudaMallocManaged(&amp;ptr, len);\n        cudaDeviceSynchronize();\n        return ptr;\n    }\n\n    void operator delete(void *ptr) {\n\n        cudaDeviceSynchronize();\n        cudaFree(ptr);\n    }\n}\n\n</code></pre> <pre><code>class umString : public Managed {\n  int length;\n  char *data;\n\npublic:\n  umString(int len) : length(len) {\n    cudaMallocManaged(&amp;data, length + 1);\n    data[length] = '\\0';\n  }\n\n  // Copy constructor\n  umString(const umString &amp;s) : length(s.length) {\n    cudaDeviceSynchronize();                 // ensure coherence\n    cudaMallocManaged(&amp;data, length + 1);\n    std::memcpy(data, s.data, length + 1);\n  }\n\n  // Destructor\n  ~umString() {\n    cudaDeviceSynchronize();\n    cudaFree(data);\n  }\n\n  // Disable assignment for safety (optional)\n  umString &amp;operator=(const umString &amp;) = delete;\n};\n\n</code></pre> <pre><code>\nclass dataElem: public Managed {\npublic:\n    int key;\n    umString name;\n}\n\ndataElem *data = new dataElem[10];\n\n</code></pre> <p>Performance Tuning </p> <ul> <li><code>cudamemPreFetchAsync</code></li> </ul>"},{"location":"languages/cuda/cuda_metrics/","title":"Cuda metrics","text":""},{"location":"languages/cuda/cuda_metrics/#cuda-metrics","title":"CUDA Metrics","text":""},{"location":"languages/cuda/cuda_metrics/#throughput","title":"Throughput","text":""},{"location":"languages/cuda/cuda_metrics/#bandwidth","title":"Bandwidth","text":""},{"location":"languages/cuda/cuda_metrics/#memory-throughput","title":"Memory Throughput","text":"<p>Refers to how fast data can be read from or written to memory (global, shared, or texture memory)</p>"},{"location":"languages/cuda/cuda_metrics/#compute-throughput","title":"Compute Throughput","text":"<p>Refers to the rate at which the GPU can execute operations (like floating-point operations, matrix multiplications, etc.).</p>"},{"location":"languages/cuda/cuda_metrics/#throughput-in-data-transfer","title":"Throughput in Data Transfer","text":"<p>When moving data between different parts of the system (like CPU to GPU or between different levels of GPU memory), data transfer throughput measures the efficiency and speed of that movement.</p>"},{"location":"languages/cuda/cuda_metrics/#throughput-vs-latency","title":"Throughput vs Latency","text":"<p>Latency is the time it takes to complete a single operation, while throughput is the rate at which operations are completed.</p> <p>Ideally, you want high throughput and low latency for maximum performance. However, there\u2019s often a tradeoff.</p>"},{"location":"languages/cuda/cuda_metrics/#bandwidth_1","title":"Bandwidth","text":""},{"location":"languages/cuda/cuda_multi_gpu/","title":"Cuda multi gpu","text":""},{"location":"languages/cuda/cuda_multi_gpu/#cuda-multi-gpu","title":"CUDA Multi-GPU","text":"<ul> <li>Multiple GPU.cu</li> </ul>"},{"location":"languages/cuda/cuda_resource/","title":"CUDA Training Resources","text":""},{"location":"languages/cuda/cuda_resource/#gpu-hardware","title":"GPU Hardware","text":"<ul> <li> <p>CUDA GPU Compute Capability</p> </li> <li> <p>Legacy CUDA GPU Compute Capability</p> </li> <li> <p>GPU Comparison Guides</p> </li> </ul>"},{"location":"languages/cuda/cuda_resource/#tutorial","title":"Tutorial","text":"<ul> <li>Nvidia CUDA Training Series<ul> <li>Youtube</li> <li>GitHub</li> <li>PPT</li> </ul> </li> </ul>"},{"location":"languages/cuda/cuda_syn/","title":"Cuda syn","text":""},{"location":"languages/cuda/cuda_syn/#cuda-concurrency","title":"CUDA Concurrency","text":"<ul> <li>Stream Overlap.cu</li> </ul>"},{"location":"languages/cuda/cuda_threads/","title":"Cuda threads","text":""},{"location":"languages/cuda/cuda_threads/#cuda-threads","title":"CUDA Threads","text":""},{"location":"languages/cuda/cuda_threads/#hardware-vs-software","title":"Hardware vs Software","text":"<pre><code>Grid\n \u2514\u2500\u2500 Block\n     \u2514\u2500\u2500 Warp (32 threads)\n         \u2514\u2500\u2500 Thread (software)\n</code></pre> <pre><code>GPU\n \u2514\u2500\u2500 SM (Streaming Multiprocessor)\n     \u2514\u2500\u2500 CUDA cores (ALUs)\n</code></pre> <ul> <li>Threads are executed by scalar processors (SP).</li> <li>Thread blocks are executed on multiprocessors (SM).</li> <li>A kernel is launched as a grid of thread blocks. </li> </ul>"},{"location":"languages/cuda/cuda_threads/#ids","title":"IDs","text":"<ul> <li>blockIdx, threadIdx   &lt;3D&gt;</li> <li>blockDim, gridDim  &lt;3D&gt;</li> </ul>"},{"location":"languages/cuda/cuda_threads/#global-thread-id","title":"Global Thread ID","text":""},{"location":"languages/cuda/cuda_threads/#grid-stride-loops","title":"Grid-Stride Loops","text":"<pre><code>\n__global__\nvoid saxpy(int n, float a, float *x, float *y)\n{\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i &lt; n) \n        y[i] = a * x[i] + y[i];\n}\n\n// Perform SAXPY on 1M elements\nsaxpy&lt;&lt;&lt;4096,256&gt;&gt;&gt;(1&lt;&lt;20, 2.0, x, y);\n\n__global__\nvoid saxpy(int n, float a, float *x, float *y)\n{\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; \n         i &lt; n; \n         i += blockDim.x * gridDim.x) \n      {\n          y[i] = a * x[i] + y[i];\n      }\n}\n</code></pre>"},{"location":"languages/cuda/cuda_threads/#warps","title":"Warps","text":"<ul> <li> <p>A warp is excuted physically in parallel (SIMD) on a multiprocessor.</p> </li> <li> <p>A thread block consists of warps.</p> </li> </ul>"},{"location":"languages/cuda/cuda_threads/#launch-configuration","title":"Launch Configuration","text":"<ul> <li> <p>instructions are issued in order</p> </li> <li> <p>a thread stalls when one of the operands isn't ready</p> </li> <li> <p>latency is hiden by switching threads</p> </li> <li>GMEM latency (&gt;100 cycles)</li> <li>arithmetic latency (&lt;100 cycles) </li> </ul>"},{"location":"languages/cuda/cuda_threads/#gpu-latency-hidding","title":"GPU Latency Hidding","text":"<ul> <li>Threads per block should be a multiple of warp size (32)</li> <li>Really small thread blocks prevent achieving good occupancy</li> <li>Really large thread blocks are less flexible</li> <li>Could generally use 128-256 threads/block </li> </ul>"},{"location":"languages/cuda/cuda_threads/#thread-metric-occupancy","title":"Thread Metric: Occupancy","text":"<ul> <li>A measure of the actual thread load in an SM, vs. peak theoretical/peak achievable</li> </ul>"},{"location":"languages/python/python/","title":"Python","text":"<ul> <li>A Guide to Using Python\u2019s Logging Module</li> </ul>"},{"location":"machine_learning/mlops/hydra/","title":"Hydra","text":"<ul> <li> <p>Why Should We Use Hydra for Effective Program Parameter Management?</p> </li> <li> <p>A Simple Demonstration of Experiment Results Tracking Using MLflow and Hydra</p> </li> </ul>"},{"location":"machine_learning/tensorflow/tensorflow/","title":"TensorFlow","text":""},{"location":"machine_learning/tensorflow/tensorflow/#summary","title":"Summary","text":"<ul> <li>Designing a Production-Ready Deep Learning Image Enhancement TensorFlow Framework: An End-to-End Software Architecture</li> </ul>"},{"location":"machine_learning/tensorflow/tensorflow/#dataset","title":"Dataset","text":"<ul> <li> <p>Writing and Reading Datasets with TFRecord and LMDB in TensorFlow</p> </li> <li> <p>Deterministic Data Augmentation in TensorFlow Using tf.data.Dataset and tf.random.Generator.from_seed</p> </li> <li> <p>A Practical Guide to Creating tf.data.Dataset in TensorFlow</p> </li> <li> <p>How to Generate a tf.data.Dataset from\u00a0Folders</p> </li> <li> <p>Understanding When to Use tf.py_function in TensorFlow</p> </li> <li> <p>Optimizing Your TensorFlow Input Pipeline: CPU vs. GPU Augmentation and How to Find the Bottleneck</p> </li> </ul>"},{"location":"machine_learning/tensorflow/tensorflow/#model","title":"Model","text":"<ul> <li> <p>Inspecting an Individual Keras Layer: Visualization and Summaries</p> </li> <li> <p>A Practical Toolkit for Debugging Keras\u00a0Models</p> </li> <li> <p>Writing a Robust Custom Keras Layer: A Practical Tutorial</p> </li> <li> <p>Saving and Loading Keras Models: Different Approaches</p> </li> <li> <p>The Three Keras Model Paradigms: A Practical Guide for TensorFlow Practitioners</p> </li> <li> <p>Netron: The Neural Network Visualizer Every ML Engineer\u00a0Needs</p> </li> <li> <p>Calling a PyTorch Model Inside a TensorFlow Keras Model: A Practical Inference-Only Pattern</p> </li> </ul>"},{"location":"machine_learning/tensorflow/tensorflow/#model-training-pipeline","title":"Model Training Pipeline","text":"<ul> <li> <p>The <code>training</code> Argument in Keras: What It Really Means and How to Use It Correctly</p> </li> <li> <p>Callbacks in TensorFlow model.fit()</p> </li> <li> <p>Understanding fit, evaluate, and predict: How Keras Models Really Work</p> </li> <li> <p>Mastering Model Training: A Guide to Custom Iterative Loops in TensorFlow</p> </li> <li> <p>The Keras Hybrid: Separating Architecture from Training Configuration for Maximum Flexibility</p> </li> <li> <p>Designing a Keras Model with Shared Weights for Training and Validation</p> </li> </ul>"},{"location":"machine_learning/tensorflow/tensorflow/#model-deployment","title":"Model Deployment","text":"<ul> <li> <p>Post-Training Quantization (PTQ) in TensorFlow Keras</p> </li> <li> <p>A Practical Guide to Quantization-Aware Training (QAT) in TensorFlow</p> </li> </ul>"},{"location":"resource/course/course/","title":"Course","text":"<ul> <li> <p>MIT OpenCourseWare</p> </li> <li> <p>edx</p> </li> </ul>"}]}